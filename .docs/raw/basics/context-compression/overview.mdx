---
title: Context Compression
sidebarTitle: Overview
description: Learn how to compress tool call results to save context space while preserving critical information.
keywords: [context compression, tool call compression, context management, token optimization]
---

<Badge icon="code-branch" color="orange">
    <Tooltip tip="Introduced in v2.2.3" cta="View release notes" href="https://github.com/agno-agi/agno/releases/tag/v2.2.3">v2.2.3</Tooltip>
</Badge>


Context Compression allows you to manage your agent context while it is running, helping the agent stay within its context window and avoid rate limits or decreases in response quality. 

Think of it like a research assistant who reads lengthy reports and gives you the key bullet points instead of the full documents.


## The Problem: Verbose Tool Results

If you are using tools with large response sizes, without compression, tool results quickly consume your context window:

| Component        | Cumulative Token Count    | Notes                          |
|------------------|---------------------------|--------------------------------|
| System Prompt    | 1,200 tokens              |                                |
| User Message     | 1,300 tokens              |                                |
| LLM Response     | 1,500 tokens              |                                |
| Tool Call 1      | 2,500 tokens              |                                |
| Tool Call 2      | 5,700 tokens              | 2,500 + 3,200 new              |
| Tool Call 3      | 8,500 tokens              | 5,700 + 2,800 new              |
| Tool Call 4      | 12,000 tokens             | 8,500 + 3,500 new              |

This quickly becomes expensive and hits context limits during complex workflows.

## The Solution: Automatic Compression

Context compression summarizes tool results after a threshold:

```
Tool Call 1: 2,500 tokens
Tool Call 2: 5,700 tokens
Tool Call 3: 8,500 tokens
[Compression triggered]
Tool Call 4: 1,300 tokens (800 compressed + 500 new)
```

**Benefits:**
- Dramatically reduced token costs
- Stay within context window limits
- Preserve critical facts and data
- Automatic compression

## How It Works

Context compression follows a simple pattern:

<Steps>
    <Step title="Enable Compression">
        Set `compress_tool_results=True` on your agent or team, or provide a `CompressionManager`. The system monitors tool call results as they come in.
    </Step>
    <Step title="Threshold Reached">
        After the threshold is reached, compression is triggered. Each uncompressed tool call result is individually summarized.
    </Step>
    <Step title="Intelligent Summarization">
        The compression model preserves key facts (numbers, dates, entities, URLs) while removing boilerplate, redundancy, and filler text.
    </Step>
    <Step title="The LLM loop continues">
        The compressed tool results are used in the next LLM executions, reducing token usage and extending the life of your context window.
    </Step>
</Steps>

<Note>
When using `arun` on `Agent` or `Team`, compression is handled asynchronously and the uncompressed tool call results are summarised concurrently.
</Note>

## Enable Compression

Turn on `compress_tool_results=True` to automatically compress tool results. This comes with a default threshold of 3 tool calls.

For example:
<CodeGroup>
```python Agent
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    compress_tool_results=True,
)

agent.print_response("Research each of the following topics: AI, Crypto, Web3, and Blockchain")
```

```python Team
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools

web_agent = Agent(
    name="Web Researcher",
    tools=[DuckDuckGoTools()],
)

team = Team(
    model=OpenAIChat(id="gpt-4o"),
    members=[web_agent],
    compress_tool_results=True,
)

team.print_response("Research each of the following topics: AI, Crypto, Web3, and Blockchain")
```
</CodeGroup>

<Info>
You can also enable `compress_tool_results=True` on individual team members to compress their tool results independently.
</Info>

## Custom Compression

Provide a [`CompressionManager`](/reference/compression/compression-manager) to customize the compression behavior:

<CodeGroup>
```python Agent
from agno.agent import Agent
from agno.compression.manager import CompressionManager
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

compression_manager = CompressionManager(
    model=OpenAIChat(id="gpt-4o-mini"),  # Use a faster model for compression
    compress_tool_results_limit=2,  # Compress after 2 tool calls (default: 3)
    compress_tool_call_instructions="Your custom compression prompt here...",
)

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    compression_manager=compression_manager,
)

agent.print_response("Find recent funding rounds for AI startups")
```

```python Team
from agno.agent import Agent
from agno.compression.manager import CompressionManager
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools

compression_manager = CompressionManager(
    model=OpenAIChat(id="gpt-4o-mini"),  # Use a faster model for compression
    compress_tool_results_limit=2,  # Compress after 2 tool calls (default: 3)
    compress_tool_call_instructions="Your custom compression prompt here...",
)

web_agent = Agent(
    name="Web Researcher",
    tools=[DuckDuckGoTools()],
)

team = Team(
    model=OpenAIChat(id="gpt-4o"),
    members=[web_agent],
    compression_manager=compression_manager,
)

team.print_response("Find recent funding rounds for AI startups")
```
</CodeGroup>

<Tip>
Use a faster, cheaper model like `gpt-4o-mini` for compression to reduce latency and cost while using a more capable model as your Agent's main model.
</Tip>

## Compression Triggers

The `CompressionManager` supports two types of thresholds for triggering compression:

| Mode | Parameter | Use Case |
|------|-----------|----------|
| **Count-Based** | `compress_tool_results_limit` | Predictable tool call patterns. Triggers after N uncompressed tool results. |
| **Token-Based** | `compress_token_limit` | Variable result sizes or strict context limits. Triggers when context exceeds a token threshold. |

<Note>
If neither threshold is set, `compress_tool_results_limit` defaults to `3`.
</Note>

### Tool-Based Compression

Set `compress_tool_results_limit` when you have predictable tool call patterns and want compression to trigger after a fixed number of tool call results.

### Token-Based Compression

Use `compress_token_limit` when you need precise control over context size, especially when tool results vary significantly in size:

<CodeGroup>
```python Agent
from agno.agent import Agent
from agno.compression.manager import CompressionManager
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

compression_manager = CompressionManager(
    model=OpenAIChat(id="gpt-4o-mini"),
    compress_tool_results=True,
    compress_token_limit=5000,  # or compress_tool_results_limit
)

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    compression_manager=compression_manager,
)

agent.print_response("Research AI companies: OpenAI, Anthropic, Google DeepMind, Meta AI")
```

```python Team
from agno.agent import Agent
from agno.compression.manager import CompressionManager
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools

compression_manager = CompressionManager(
    model=OpenAIChat(id="gpt-4o-mini"),
    compress_tool_results=True,
    compress_token_limit=5000,  # or compress_tool_results_limit
)

web_agent = Agent(
    name="Web Researcher",
    tools=[DuckDuckGoTools()],
)

team = Team(
    model=OpenAIChat(id="gpt-4o"),
    members=[web_agent],
    compression_manager=compression_manager,
)

team.print_response("Research AI companies: OpenAI, Anthropic, Google DeepMind, Meta AI")
```
</CodeGroup>

<Info>
Token counting includes messages, tool definitions, and output schemas. See [Token Counting](/basics/context-compression/token-counting) for details.
</Info>

## When to Use Context Compression

**Perfect for:**
- Agents with tools that return verbose results (web search, APIs)
- Multi-step workflows with many tool calls
- Long-running sessions where context accumulates
- Production systems where cost matters


## Developer Resources

- [CompressionManager Reference](/reference/compression/compression-manager) - Full CompressionManager documentation
- [Agent Reference](/reference/agents/agent) - Agent parameter documentation
- [Team Reference](/reference/teams/team) - Team parameter documentation
- [Cookbook Examples](https://github.com/agno-agi/agno/tree/main/cookbook/03_agents/context_compression)
