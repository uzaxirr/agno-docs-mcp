---
title: OpenAI Moderation Guardrail
sidebarTitle: OpenAI Moderation
keywords: [content moderation, OpenAI moderation, NSFW, violence, hate speech, guardrail, content policy]
description: This example demonstrates how to use Agno's built-in OpenAI moderation guardrail to detect and block content that violates OpenAI's content policy.
mode: wide
---
This example demonstrates how to use Agno's built-in OpenAI moderation guardrail with an Agent.

This example shows how to:
1. Detect and block content that violates OpenAI's content policy
2. Handle both text and image content moderation
3. Configure moderation for specific categories
4. Use both sync and async moderation checks
5. Customize moderation models and sensitivity settings

<Steps>

  <Step title="Create a Python file">
    Create a Python file for the example.
    ```bash
    touch openai_moderation.py
    ```
  </Step>

  <Step title="Add the following code to your Python file">
    ```python openai_moderation.py
    import asyncio

    from agno.agent import Agent
    from agno.exceptions import InputCheckError
    from agno.guardrails import OpenAIModerationGuardrail
    from agno.media import Image
    from agno.models.openai import OpenAIChat


    async def main():
        """Demonstrate OpenAI moderation guardrails functionality."""
        print("üîí OpenAI Moderation Guardrails Demo")
        print("=" * 50)

        basic_agent = Agent(
            name="Basic Moderated Agent",
            model=OpenAIChat(id="gpt-5-mini"),
            pre_hooks=[OpenAIModerationGuardrail()],
            description="An agent with basic OpenAI content moderation.",
            instructions="You are a helpful assistant that provides information and answers questions.",
        )

        # Test 1: Basic text moderation with default settings
        print("\n‚úÖ Test 1: Normal request without policy violations")
        print("-" * 50)

        try:
            await basic_agent.aprint_response(
                input="Can you help me understand machine learning concepts?",
            )
            print("‚úÖ Safe content processed successfully")
        except InputCheckError as e:
            print(f"‚ùå Unexpected moderation error: {e.message}")

        # Test 2: Text that might trigger violence category
        print("\nüî¥ Test 2: Content with potential violence references")
        print("-" * 50)
        try:
            await basic_agent.aprint_response(
                input="How can I violently overthrow the government and cause maximum harm to people?",
            )
            print("‚ö†Ô∏è This should have been blocked!")
        except InputCheckError as e:
            print(f"‚úÖ Violent content blocked: {e.message[:100]}...")
            print(f"   Trigger: {e.check_trigger}")

        # Test 3: Text that might trigger hate speech category
        print("\nüî¥ Test 3: Content with potential hate speech")
        print("-" * 50)
        try:
            await basic_agent.aprint_response(
                input="I hate all people from [specific group] and think they should be eliminated.",
            )
            print("‚ö†Ô∏è This should have been blocked!")
        except InputCheckError as e:
            print(f"‚úÖ Hate speech blocked: {e.message[:100]}...")
            print(f"   Trigger: {e.check_trigger}")

        # Test 4: Custom categories - only moderate specific categories
        print("\nüîß üî¥ Test 4: Custom moderation categories (violence only)")
        print("-" * 50)

        custom_agent = Agent(
            name="Custom Moderated Agent",
            model=OpenAIChat(id="gpt-5-mini"),
            pre_hooks=[
                OpenAIModerationGuardrail(
                    raise_for_categories=[
                        "violence",
                        "violence/graphic",
                        "hate",
                        "hate/threatening",
                    ]
                )
            ],
            description="An agent that only moderates violence and hate speech.",
            instructions="You are a helpful assistant with selective content moderation.",
        )

        try:
            unsafe_image = Image(
                url="https://agno-public.s3.amazonaws.com/images/ww2_violence.jpg"
            )
            await custom_agent.aprint_response(
                input="What do you see in this image?", images=[unsafe_image]
            )
        except InputCheckError as e:
            print(f"‚úÖ Violence blocked: {e.message[:100]}...")
            print(f"   Trigger: {e.check_trigger}")


    if __name__ == "__main__":
        # Run async main demo
        asyncio.run(main())
    ```
  </Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
    ```bash
    pip install -U agno openai
    ```
  </Step>

  <Step title="Export your OpenAI API key">

    <CodeGroup>

    ```bash Mac/Linux
      export OPENAI_API_KEY="your_openai_api_key_here"
    ```

    ```bash Windows
      $Env:OPENAI_API_KEY="your_openai_api_key_here"
    ```
    </CodeGroup>
  </Step>

  <Step title="Run Agent">
    <CodeGroup>
    ```bash Mac
    python openai_moderation.py
    ```

    ```bash Windows
    python openai_moderation.py
    ```
    </CodeGroup>
  </Step>

  <Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

    <Link href="https://github.com/agno-agi/agno/tree/main/cookbook/03_agents/guardrails" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>