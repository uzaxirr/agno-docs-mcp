---
title: Streaming Agent
---

Make sure to start the proxy server:

```shell
litellm --model gpt-5-mini --host 127.0.0.1 --port 4000
```

## Code

```python cookbook/11_models/litellm_openai/basic_stream.py
from agno.agent import Agent, RunOutput  # noqa
from agno.models.litellm import LiteLLMOpenAI

agent = Agent(model=LiteLLMOpenAI(id="gpt-5-mini"), markdown=True)

agent.print_response("Share a 2 sentence horror story", stream=True)
```

## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
    ```bash
    export LITELLM_API_KEY=xxx
    ```
  </Step>

  <Step title="Install libraries">
    ```bash
    pip install -U litellm[proxy] openai agno
    ```
  </Step>

  <Step title="Start the proxy server">
    ```bash
    litellm --model gpt-5-mini --host 127.0.0.1 --port 4000
    ```
  </Step>

  <Step title="Run Agent">
    <CodeGroup>
    ```bash Mac
    python cookbook/11_models/litellm/basic_stream.py
    ```

    ```bash Windows
    python cookbook/11_models/litellm/basic_stream.py
    ```
    </CodeGroup>
  </Step>
</Steps>