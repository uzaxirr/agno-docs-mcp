---
title: Claude
sidebarTitle: Claude
---

The Claude model provides access to Anthropic's Claude models.

## Parameters

| Parameter                       | Type                        | Default                        | Description                                                     |
| ------------------------------ | --------------------------- | ------------------------------ | --------------------------------------------------------------- |
| `id`                           | `str`                       | `"claude-3-5-sonnet-20241022"` | The id of the Anthropic Claude model to use                     |
| `name`                         | `str`                       | `"Claude"`                     | The name of the model                                           |
| `provider`                     | `str`                       | `"Anthropic"`                  | The provider of the model                                       |
| `max_tokens`                   | `Optional[int]`             | `4096`                         | Maximum number of tokens to generate in the chat completion     |
| `thinking`                     | `Optional[Dict[str, Any]]`  | `None`                         | Configuration for the thinking (reasoning) process           |
| `temperature`                  | `Optional[float]`           | `None`                         | Controls randomness in the model's output                       |
| `stop_sequences`               | `Optional[List[str]]`       | `None`                         | A list of strings that the model should stop generating text at |
| `top_p`                        | `Optional[float]`           | `None`                         | Controls diversity via nucleus sampling                         |
| `top_k`                        | `Optional[int]`             | `None`                         | Controls diversity via top-k sampling                           |
| `cache_system_prompt`          | `Optional[bool]`            | `False`                        | Whether to cache the system prompt for improved performance     |
| `extended_cache_time`          | `Optional[bool]`            | `False`                        | Whether to use extended cache time (1 hour instead of default)  |
| `request_params`               | `Optional[Dict[str, Any]]`  | `None`                         | Additional parameters to include in the request                 |
| `mcp_servers`                  | `Optional[List[MCPServerConfiguration]]` | `None`        | List of MCP (Model Context Protocol) server configurations      |
| `api_key`                      | `Optional[str]`             | `None`                         | The API key for authenticating with Anthropic                   |
| `default_headers`              | `Optional[Dict[str, Any]]`  | `None`                         | Default headers to include in all requests                      |
| `client_params`                | `Optional[Dict[str, Any]]`  | `None`                         | Additional parameters for client configuration                  |
| `client`                       | `Optional[AnthropicClient]` | `None`                         | A pre-configured instance of the Anthropic client               |
| `async_client`                 | `Optional[AsyncAnthropicClient]` | `None`                    | A pre-configured instance of the async Anthropic client         |
| `retries`                      | `int`                       | `0`                            | Number of retries to attempt before raising a ModelProviderError |
| `delay_between_retries`        | `int`                       | `1`                            | Delay between retries, in seconds                               |
| `exponential_backoff`          | `bool`                      | `False`                        | If True, the delay between retries is doubled each time         |