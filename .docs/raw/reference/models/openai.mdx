---
title: OpenAI
sidebarTitle: OpenAI
---

The OpenAIChat model provides access to OpenAI models like GPT-4o.

## Parameters

| Parameter                    | Type                           | Default                | Description                                                                                              |
| ---------------------------- | ------------------------------ | ---------------------- | -------------------------------------------------------------------------------------------------------- |
| `id`                         | `str`                          | `"gpt-4o"`             | The id of the OpenAI model to use                                                                       |
| `name`                       | `str`                          | `"OpenAIChat"`         | The name of the model                                                                                    |
| `provider`                   | `str`                          | `"OpenAI"`             | The provider of the model                                                                                |
| `store`                      | `Optional[bool]`               | `None`                 | Whether to store the conversation for training purposes                                                  |
| `reasoning_effort`           | `Optional[str]`                | `None`                 | The reasoning effort level for o1 models ("low", "medium", "high")                                      |
| `verbosity`                  | `Optional[Literal["low", "medium", "high"]]` | `None`     | Controls verbosity level of reasoning models                                                             |
| `metadata`                   | `Optional[Dict[str, Any]]`     | `None`                 | Developer-defined metadata to associate with the completion                                              |
| `frequency_penalty`          | `Optional[float]`              | `None`                 | Penalizes new tokens based on their frequency in the text so far (-2.0 to 2.0)                         |
| `logit_bias`                 | `Optional[Any]`                | `None`                 | Modifies the likelihood of specified tokens appearing in the completion                                  |
| `logprobs`                   | `Optional[bool]`               | `None`                 | Whether to return log probabilities of the output tokens                                                 |
| `top_logprobs`               | `Optional[int]`                | `None`                 | Number of most likely tokens to return log probabilities for (0 to 20)                                  |
| `max_tokens`                 | `Optional[int]`                | `None`                 | Maximum number of tokens to generate (deprecated, use max_completion_tokens)                            |
| `max_completion_tokens`      | `Optional[int]`                | `None`                 | Maximum number of completion tokens to generate                                                          |
| `modalities`                 | `Optional[List[str]]`          | `None`                 | List of modalities to use ("text" and/or "audio")                                                       |
| `audio`                      | `Optional[Dict[str, Any]]`     | `None`                 | Audio configuration (e.g., `{"voice": "alloy", "format": "wav"}`)                                         |
| `presence_penalty`           | `Optional[float]`              | `None`                 | Penalizes new tokens based on whether they appear in the text so far (-2.0 to 2.0)                     |
| `seed`                       | `Optional[int]`                | `None`                 | Random seed for deterministic sampling                                                                   |
| `stop`                       | `Optional[Union[str, List[str]]]` | `None`              | Up to 4 sequences where the API will stop generating further tokens                                     |
| `temperature`                | `Optional[float]`              | `None`                 | Controls randomness in the model's output (0.0 to 2.0)                                                  |
| `user`                       | `Optional[str]`                | `None`                 | A unique identifier representing your end-user                                                           |
| `top_p`                      | `Optional[float]`              | `None`                 | Controls diversity via nucleus sampling (0.0 to 1.0)                                                    |
| `service_tier`               | `Optional[str]`                | `None`                 | Service tier to use ("auto", "default", "flex", "priority")                                             |
| `strict_output`              | `bool`                         | `True`                 | Controls schema adherence for structured outputs                                                         |
| `extra_headers`              | `Optional[Any]`                | `None`                 | Additional headers to include in requests                                                                |
| `extra_query`                | `Optional[Any]`                | `None`                 | Additional query parameters to include in requests                                                       |
| `extra_body`                 | `Optional[Any]`                | `None`                 | Additional body parameters to include in requests                                                        |
| `request_params`             | `Optional[Dict[str, Any]]`     | `None`                 | Additional parameters to include in the request                                                          |
| `role_map`                   | `Optional[Dict[str, str]]`     | `None`                 | Mapping of message roles to OpenAI roles                                                                |
| `api_key`                    | `Optional[str]`                | `None`                 | The API key for authenticating with OpenAI (defaults to OPENAI_API_KEY env var)                        |
| `organization`               | `Optional[str]`                | `None`                 | The organization ID to use for requests                                                                  |
| `base_url`                   | `Optional[Union[str, httpx.URL]]` | `None`              | The base URL for the OpenAI API                                                                         |
| `timeout`                    | `Optional[float]`              | `None`                 | Request timeout in seconds                                                                               |
| `max_retries`                | `Optional[int]`                | `None`                 | Maximum number of retries for failed requests                                                           |
| `default_headers`            | `Optional[Any]`                | `None`                 | Default headers to include in all requests                                                               |
| `default_query`              | `Optional[Any]`                | `None`                 | Default query parameters to include in all requests                                                     |
| `http_client`                | `Optional[Union[httpx.Client, httpx.AsyncClient]]` | `None` | HTTP client instance for making requests                                                  |
| `client_params`              | `Optional[Dict[str, Any]]`     | `None`                 | Additional parameters for client configuration                                                           |
| `retries`                    | `int`                          | `0`                    | Number of retries to attempt before raising a ModelProviderError                                        |
| `delay_between_retries`      | `int`                          | `1`                    | Delay between retries, in seconds                                                                       |
| `exponential_backoff`        | `bool`                         | `False`                | If True, the delay between retries is doubled each time                                                 |
